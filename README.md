# Large Language Models in Financial Decision-Making

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Research](https://img.shields.io/badge/Type-Research_Framework-purple.svg)](#research-questions)

**An empirical framework for evaluating AI agents in financial markets**

This research framework provides rigorous methodological tools to assess Large Language Model performance against established quantitative trading strategies. The framework enables systematic investigation of memory adaptation, probabilistic calibration, and behavioral patterns in AI-driven financial decision-making.

---

## ğŸ¯ What This Does

### The Trading Process
Each day, the LLM analyzes market data and **chooses one action**:
- **BUY**: Take a long position (+1.0) - profit if market goes up
- **HOLD**: Stay in cash (0.0) - no profit/loss regardless of market movement
- **SELL**: Take a short position (-1.0) - profit if market goes down

The LLM receives technical indicators, can maintain a strategic journal, and may express emotional states. Performance is rigorously compared against traditional quantitative strategies.

### Research Capabilities
- **ğŸ§ª Compare LLMs vs Traditional Strategies**: Test any LLM against momentum, mean-reversion, volatility timing, and other proven approaches
- **ğŸ§  Memory & Adaptation**: Study how LLMs learn from experience with strategic journals, feeling logs, and optional complete trading history
- **ğŸ¯ Calibration Analysis**: Assess if LLM confidence matches actual performance
- **ğŸ§® Behavioral Biases**: Detect human-like trading biases in AI decisions
- **ğŸ“Š Statistical Rigor**: Bootstrap testing, out-of-sample validation, dual-criteria HOLD evaluation

---

## ğŸ”¬ Research Objectives

### Memory and Learning Dynamics
**Investigation of temporal learning in sequential financial decisions**
- Analysis of historical context integration in LLM decision processes
- Assessment of adaptive behavior based on performance feedback
- Evaluation of emotional state impacts on decision quality

### Probabilistic Calibration
**Assessment of confidence-outcome alignment in AI predictions**
- Measurement of overconfidence/underconfidence patterns
- Decision-specific calibration analysis (BUY/SELL/HOLD)
- Longitudinal calibration stability assessment

### Behavioral Pattern Analysis
**Detection of systematic biases in AI trading behavior**
- Loss aversion quantification in position management
- Disposition effect identification in profit-taking behavior
- Risk management appropriateness in uncertain conditions

---

## ğŸ“Š At a Glance

| Component | Status | Description |
|-----------|--------|-------------|
| **LLM Integration** | âœ… OpenRouter API | BERT, GPT, Claude, Gemini models |
| **Baseline Strategies** | âœ… 7 Strategies | Momentum, Mean Reversion, Volatility Timing |
| **Statistical Validation** | âœ… Bootstrap Testing | Out-of-sample, significance testing |
| **Decision Analysis** | âœ… Dual-Criteria | Calibration + behavioral pattern analysis |
| **Reporting** | âœ… Comprehensive | Automated research reports |

---

## ğŸš€ Getting Started

### Modern Installation (Recommended)
```bash
# Install everything (core + development tools)
pip install -e .[dev]

# Or install core dependencies only
pip install -e .
```

### Quick Test Run
```bash
# Run with dummy model (no API key needed)
python -m src.main
```

### Development Workflow
```bash
# Run quality checks (linting, testing, type checking)
python scripts/dev-workflow.py check

# Windows users can also use:
dev check

# Linux/Mac users can also use:
make check
```

### Real LLM Experiment
```bash
# 1. Get OpenRouter API key from https://openrouter.ai/
#    See documentation: https://openrouter.ai/docs
export OPENROUTER_API_KEY="your_key_here"

# 2. Configure your experiment in src/config.py
TEST_MODE = False  # Set to False for full dataset (default: ~2700 days)
# Or customize: DAYS_TO_RUN = 100  # If keeping TEST_MODE = True

# 3. Choose LLM model
ACTIVE_MODEL = "bert"  # or other models

# 4. Run experiment
python -m src.main
```

### Traditional Installation (Alternative)
```bash
# Core dependencies
pip install pandas numpy scipy matplotlib requests

# Development dependencies (optional)
pip install pytest black flake8 mypy isort
```

## ğŸ› ï¸ Development Tools

### Cross-Platform Commands
```bash
# Quality assurance
python scripts/dev-workflow.py check    # Full check suite
python scripts/dev-workflow.py test     # Run tests only
python scripts/dev-workflow.py lint     # Code style checks

# Version management
python scripts/version.py get           # Current version
python scripts/version.py bump patch    # Bump patch version
python scripts/version.py tag           # Create git tag

# Collaboration helpers
python scripts/collaborate.py status    # Repository status
python scripts/collaborate.py branch    # Create feature branch
python scripts/collaborate.py pr        # Prepare for PR
```

### Windows Development
```batch
dev setup     # Install all dependencies
dev check     # Run quality checks
dev test      # Run tests
dev lint      # Check code style
dev clean     # Clean build artifacts
dev collab    # Collaboration tools
```

### Linux/Mac Development
```bash
make setup    # Install all dependencies
make check    # Run quality checks
make test     # Run tests
make lint     # Check code style
make clean    # Clean build artifacts
```

### Automated Quality Assurance
Every commit is automatically checked by our CI/CD pipeline:
- âœ… **Code Linting**: flake8, black, isort
- âœ… **Type Checking**: mypy validation
- âœ… **Testing**: pytest with coverage reporting
- âœ… **Import Validation**: All dependencies properly declared

**âš ï¸ Important**: Set `TEST_MODE = False` in `src/config.py` for meaningful results, or adjust `DAYS_TO_RUN` if keeping test mode.

## ğŸ“Š Sample Results (Dummy Model)

*These are comprehensive test results from a 1000-day simulation using a dummy model - run with real LLMs for actual research!*

### Strategy Performance Comparison
![Baseline Comparison](example_results/plots/dummy_model_memory_feeling_baseline_comparison.png)
*LLM strategies vs traditional approaches (dummy data for illustration)*

### Decision Calibration Analysis
![Calibration by Decision](example_results/plots/dummy_model_memory_feeling_calibration_by_decision.png)
*Confidence vs actual performance across BUY/HOLD/SELL decisions*

### Behavioral Pattern Analysis
![Decision Patterns](example_results/plots/dummy_model_memory_feeling_decision_patterns.png)
*Decision changes after wins vs losses - evidence of learning/adaptation*

### Risk Analysis
![Risk Analysis](example_results/plots/dummy_model_memory_feeling_risk_analysis.png)
*Comprehensive risk metrics including VaR, drawdowns, and stress testing*

### Rolling Performance
![Rolling Performance](example_results/plots/dummy_model_memory_feeling_rolling_performance.png)
*Rolling Sharpe ratio and performance metrics over different time windows*

## ğŸ“ˆ Data Source

**Stock data provided by Stooq**: Access financial data at [stooq.com](https://stooq.com/)

The framework is designed to work with any daily stock/index data. Modify `src/data_prep.py` to integrate your preferred data source.

---

---

## ğŸ—ï¸ Architecture

```
ğŸ“ Root Level
â”œâ”€â”€ ğŸ“– README.md               # Main project documentation
â”œâ”€â”€ ğŸ“¦ pyproject.toml          # Modern Python packaging & metadata
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Core dependency list
â”œâ”€â”€ ğŸ§ Makefile               # Unix/Linux development commands
â”œâ”€â”€ ğŸªŸ dev.bat                 # Windows development commands
â”œâ”€â”€ âš–ï¸ LICENSE                 # Project license
â”œâ”€â”€ ğŸ“ CHANGELOG.md           # Version history & changes
â””â”€â”€ ğŸ¤ CODE_OF_CONDUCT.md     # Community standards

ğŸ“ src/
â”œâ”€â”€ ğŸ¯ main.py                 # Pipeline orchestrator
â”œâ”€â”€ ğŸ¤– openrouter_model.py     # LLM API integration (BERT primary)
â”œâ”€â”€ ğŸ“Š backtest.py            # Strategy evaluation engine
â”œâ”€â”€ ğŸ§® baselines.py           # 7 traditional quantitative strategies
â”œâ”€â”€ ğŸ“ˆ statistical_validation.py # Bootstrap testing + out-of-sample
â”œâ”€â”€ ğŸ“‹ report_generator.py    # Automated research reports
â”œâ”€â”€ ğŸ“Š reporting.py           # Plotting & visualization
â”œâ”€â”€ âš™ï¸ config.py              # 6 experimental configurations
â”œâ”€â”€ ğŸ”§ data_prep.py           # Data preprocessing utilities
â”œâ”€â”€ ğŸ¯ decision_analysis.py   # Decision pattern analysis
â”œâ”€â”€ ğŸ¤– dummy_model.py         # Mock model for testing
â”œâ”€â”€ ğŸ“‹ prompts.py             # LLM prompt templates
â””â”€â”€ ğŸ¤– trading_engine.py      # Main experiment orchestration

ğŸ“ scripts/
â”œâ”€â”€ ğŸ·ï¸ version.py             # Version management system
â”œâ”€â”€ ğŸ”„ dev-workflow.py        # Development automation
â”œâ”€â”€ ğŸ¤ collaborate.py         # Collaboration helpers
â””â”€â”€ ğŸ“‹ generate_report.py     # Report generation utility

ğŸ“ docs/
â”œâ”€â”€ ğŸ¤ COLLABORATION_GUIDE.md # Detailed collaboration guide
â”œâ”€â”€ âš™ï¸ configuration.md       # Configuration reference
â”œâ”€â”€ ğŸ”¬ methodology.md         # Research methodology
â””â”€â”€ ğŸ“š STRATEGIC_JOURNAL_FEATURE.md # Memory system details

ğŸ“ tests/
â”œâ”€â”€ ğŸ§ª test_calibration.py    # Calibration testing
â””â”€â”€ ğŸ§ª test_strategic_journal_config.py # Configuration testing

ğŸ“ data/
â”œâ”€â”€ ğŸ“¥ raw/                   # Raw financial data (tracked)
â””â”€â”€ ğŸ“¤ processed/             # Processed data (gitignored)

ğŸ“ example_results/           # ğŸ“‹ Example outputs for documentation
ğŸ“ results/                   # ğŸ“Š Generated results (gitignored)

ğŸ“ .github/
â”œâ”€â”€ ğŸ¤– workflows/ci.yml       # CI/CD pipeline
â”œâ”€â”€ ğŸ”„ dependabot.yml         # Automated dependency updates
â”œâ”€â”€ ğŸ“‹ PULL_REQUEST_TEMPLATE.md # PR guidelines
â””â”€â”€ ğŸ› ISSUE_TEMPLATE/         # Issue templates

ğŸ“ results/
â”œâ”€â”€ ğŸ“Š analysis/              # Statistical validation JSON/CSV
â”œâ”€â”€ ğŸ“ˆ plots/                 # Calibration, patterns, risk analysis
â”œâ”€â”€ ğŸ“‹ reports/               # Comprehensive experiment reports
â””â”€â”€ ğŸ“„ parsed/                # Processed trading decisions

ğŸ“ tests/
â”œâ”€â”€ ğŸ§ª test_calibration.py    # Calibration testing
â””â”€â”€ ğŸ§ª test_strategic_journal_config.py # Configuration testing
```

---

## ğŸ“ˆ Trading Strategies

### ğŸ¤– LLM Strategy Configurations

| Configuration | Memory | Feelings | Dates | Purpose |
|---------------|--------|----------|-------|---------|
| **baseline** | âŒ | âŒ | âŒ | Pure LLM without context |
| **memory_only** | âœ… | âŒ | âŒ | Strategic journal learning |
| **memory_feeling** | âœ… | âœ… | âŒ | Emotional state tracking |
| **dates_memory** | âœ… | âŒ | âœ… | Temporal awareness âš ï¸ |
| **dates_full** | âœ… | âœ… | âœ… | Complete context âš ï¸ |

**âš ï¸ Research Note**: Date-enabled configurations shouldn't be used if you wan to prevent data leakage from historical knowledge contamination.

### ğŸ¯ Baseline Strategies
| Strategy | Logic | Parameters | 2015-2017 Return |
|----------|-------|------------|------------------|
| **Buy & Hold** | Passive index | - | +14.2% |
| **Momentum** | Trend following | 20-day MA | +5.1% |
| **Mean Reversion** | Buy dips | Â±2Ïƒ thresholds | +5.5% |
| **Volatility Timing** | Risk management | 20% vol threshold | +11.1% |
| **Contrarian** | Fade extremes | Inverse signals | +9.1% |
| **Random** | Statistical baseline | Equal weights | -0.03% |

---

## ğŸ“Š Statistical Validation Framework

### Bootstrap Significance Testing
```python
# Rigorous statistical comparison
bootstrap_results = bootstrap_sharpe_comparison(
    llm_returns, benchmark_returns, n_bootstrap=5000
)

print(f"p-value: {bootstrap_results['p_value_two_sided']:.4f}")
print(f"95% CI: [{bootstrap_results['ci_95_bootstrap'][0]:+.3f}, {bootstrap_results['ci_95_bootstrap'][1]:+.3f}]")
```

**Key Features:**
- âœ… 5,000 bootstrap resamples for robust significance testing
- âœ… Confidence intervals account for return distribution non-normality
- âœ… Effect sizes (Cohen's d) for practical significance
- âœ… Multiple testing corrections

### Out-of-Sample Validation
```
OUT-OF-SAMPLE VALIDATION:
  Train Period: -0.060 Sharpe (750 periods: 2015-2018)
  Test Period:  -0.060 Sharpe (250 periods: 2018-2019)
  Sharpe Decay: 0.0% reduction ğŸš¨ OVERFITTING DETECTED
  Confidence: 95% statistical validation with 5,000 bootstrap samples
```

### HOLD Decision Analysis (Dual-Criteria)
```
HOLD DECISION ANALYSIS:
Overall Success Rate: 40.4% (POOR)

Quiet Market Success (<0.2% moves): 0.6% âœ… Appropriate caution
Contextual Correctness: 100.0% âœ… Right conditions
Combined Score: 40.4% âš ï¸ Too conservative overall
```

---

## ğŸ“‹ Sample Results

### Performance Comparison
```
STRATEGY COMPARISON - dummy_model_memory_feeling
================================================================================
Strategy                 Return    Sharpe   MaxDD   Win%
buy_and_hold             30.5%     0.562   -33.9%   54.2
momentum                 25.8%     0.489   -25.1%   51.3
LLM_STRATEGY             -2.6%     -0.060  -45.2%   48.7 â—„
random_mean (n=30)       -15.4%     N/A     N/A     N/A
================================================================================
```

### Statistical Significance
```
BOOTSTRAP TEST VS INDEX:
  Strategy Sharpe: -0.060
  Index Sharpe: 0.562
  Difference: -0.622
  p-value: 0.384 (NOT SIGNIFICANT)
  95% CI: [-2.000, +0.754]
```

### HOLD Analysis
```
HOLD DECISION ANALYSIS
Overall HOLD Success Rate: 40.0% (GOOD)

Quiet Market Success (<0.2% Daily Moves): 40.0%
Contextual Decision Correctness: 100.0%
```

---

## ğŸ¨ Generated Visualizations

- **ğŸ“Š Baseline Comparisons**: Bar charts, equity curves
- **ğŸ¯ Calibration Plots**: Prediction accuracy, confidence analysis
- **ğŸ“ˆ Risk Analysis**: VaR curves, drawdown timelines, stress tests
- **ğŸ“‹ Decision Patterns**: Win rates by scenario, position duration
- **ğŸ“‰ Rolling Performance**: Sharpe evolution, volatility tracking

---

## ğŸ”¬ Methodology Details

### Experimental Design
- **Data**: Daily stock/index prices (customizable period and source)
- **LLM Models**: BERT, GPT, Claude, Gemini via OpenRouter API
- **Evaluation**: Bootstrap significance testing, out-of-sample validation
- **Backtesting**: Configurable assumptions and transaction costs

### Statistical Framework
- **Primary Metric**: Risk-adjusted returns (Sharpe ratio)
- **Significance Level**: p < 0.05 (bootstrap tests)
- **Validation**: Chronological train/test splits
- **HOLD Evaluation**: Dual-criteria assessment (quiet markets + context)

### Decision Analysis Pipeline
1. **LLM Response Parsing** â†’ Decision + Probability + Explanation + Journal
2. **Calibration Assessment** â†’ Confidence vs Reality analysis
3. **Pattern Recognition** â†’ Behavioral pattern detection
4. **HOLD Evaluation** â†’ Dual-criteria success assessment
5. **Statistical Validation** â†’ Bootstrap significance testing

---

## ğŸ“– Usage Examples

### Run Complete Experiment
```bash
# Configure in src/config.py
ACTIVE_EXPERIMENT = "memory_only"  # or "baseline", "dates_memory", etc.

# Run pipeline
python -m src.main
```

### Analyze Specific Model
```python
from src.report_generator import generate_comprehensive_report

# Generate full research report
report_path = generate_comprehensive_report("bert_memory_only")
print(f"Report saved: {report_path}")
```

### Statistical Validation
```python
from src.statistical_validation import comprehensive_statistical_validation

# Run full validation suite
validation = comprehensive_statistical_validation(parsed_df, model_tag)
print(f"LLM vs Index p-value: {validation['bootstrap_vs_index']['p_value']}")
```

---



## ğŸ”¬ Future Research Directions

### Immediate Extensions
- **Technical Indicators**: Implement RSI, MACD, and other momentum oscillators for richer market signals
- **Enhanced Feature Engineering**: Improve prompt engineering with new technical indicators and market regime detection
- **Cross-Model Comparison**: BERT vs GPT vs Claude calibration differences
- **Market Regime Analysis**: Calibration stability across bull/bear cycles


### Long-Term Questions
- **AI Behavioral Training**: Can we train LLMs to avoid human cognitive biases?
- **Hybrid Systems**: Combining LLM intuition with quantitative risk management
- **Real-Time Adaptation**: Online learning from live trading feedback
- **Multi-Asset Extension**: Beyond single-stock to portfolio optimization

---

## ğŸ“– Documentation

- **[Configuration Guide](docs/configuration.md)**: Complete setup and configuration options
- **[Research Methodology](docs/methodology.md)**: Statistical framework and experimental design
- **[Contributing Guide](CONTRIBUTING.md)**: How to contribute to the project
- **[Strategic Journal Feature](docs/STRATEGIC_JOURNAL_FEATURE.md)**: Memory system implementation details

## ğŸ¤ Contributing to Research

We welcome contributions from researchers, developers, and financial practitioners. See our detailed [Contributing Guide](CONTRIBUTING.md) for comprehensive information.

### Quick Start for Contributors
- **ğŸ“– Read the [Contributing Guide](CONTRIBUTING.md)** for detailed instructions
- **ğŸ”¬ Research**: Test new LLMs, develop strategies, enhance statistical methods
- **ğŸ’» Code**: Bug fixes, features, performance improvements
- **ğŸ§ª Testing**: Add test coverage, validate results
- **ğŸ“š Docs**: Improve documentation and tutorials

### Research Opportunities
- **Model Benchmarking**: Compare BERT, Claude, Gemini, and emerging LLMs
- **Strategy Development**: New baseline strategies and prompting techniques
- **Market Analysis**: Test across different markets and time periods
- **Behavioral Research**: Develop new AI bias detection methods

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **OpenRouter** for LLM API access
- **Stooq** for providing historical financial data

---

This framework provides methodological tools for systematic investigation of artificial intelligence in financial decision-making, integrating computational methods with behavioral economics and quantitative finance.
